{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear 2:  gradiente descendente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizado e otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada uma função *target* desconhecida $f:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ e uma hipótese $h_{\\mathbf{w}}:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ o erro de $h_{\\mathbf{w}}$ é definido por:\n",
    "\n",
    "\\begin{equation}\n",
    "E_{out}(h_{\\mathbf{w}}) = \\mathbb{E}_{\\mathbf{x}\\sim p_{data}}L(h(\\mathbf{x}; \\mathbf{w}), \\; f(\\mathbf{x}))\n",
    "\\end{equation}\n",
    "\n",
    "em que $p_{data}$ é a distribuição geradora dos dados, $L$  é alguma função de perda (por exemplo, o quadrado da diferença) e $h(\\mathbf{x}; \\mathbf{w}) = h_{\\mathbf{w}}(\\mathbf{x})$. Se tivéssemos acesso a $p_{data}$ poderíamos calcular a função acima para qualquer $h_{\\mathbf{w}}$ e escolher aquele com erro mínimo.\n",
    "\n",
    "Como não temos acesso a $p_{data}$, define-se\n",
    "\n",
    "\\begin{equation}\n",
    "E_{in}(h_{\\mathbf{w}}) = J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $N$ é o tamanho do dataset de treinamento e $y_{i} = f(\\mathbf{x}_{i})$.\n",
    "\n",
    "Como visto em aula, uma relação entre $E_{in}$ e $E_{out}$ pode ser estabelecida pela **Inequação de Hoeffding**.\n",
    "\n",
    "Aqui estamos interessados em encontrar $h_{\\mathbf{w}}$ com erro $J(\\mathbf{w})$ mínimo. Isto corresponde a determinar o ponto mínimo da função $J(\\mathbf{w})$. Por isso, vamos nos concentrar apenas em uma técnica de otimização para minimizar $E_{in}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente ascendente e gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados $\\mathbf{w}, \\mathbf{u} \\in \\mathbb{R}^{d}$ e $J:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ tal que $||\\mathbf{u}||_{2} = 1$ a taxa de variação de $J$ no ponto $\\mathbf{w}$ em direção a $\\mathbf{u}$ é chamada de **derivada direcional**, $D_{\\mathbf{u}}J(\\mathbf{w})$. Definindo $g(h) = J(\\mathbf{w} + h\\mathbf{u})$, podemos usar a regra da cadeia e a definição de produto escalar para mostrar que  \n",
    "\n",
    "\\begin{equation}\n",
    "D_{\\mathbf{u}}J(\\mathbf{w}) \\;\\;=\\;\\; \\left. \\frac{dg}{dh} \\right|_{h=0} \\;\\;=\\;\\; \\mathbf{u}^{T}\\nabla_{\\mathbf{w}}J(\\mathbf{w})\n",
    "    \\;\\;=\\;\\; ||\\mathbf{u}||_{2}||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta \\;\\;=\\;\\; ||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\theta$ é o ângulo entre $\\mathbf{u}$ e $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Assim, pensando em $D_{\\mathbf{u}}J(\\mathbf{w})$ em termos do vetor $\\mathbf{u}$ temos que: $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor máximo quando $\\mathbf{u}$ tem a mesma direção que $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=0$); similarmente $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor mínimo quando $\\mathbf{u}$ tem a direção oposta a $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=\\pi$).\n",
    "\n",
    "Desse modo podemos maximizar $J$ alterando $\\mathbf{w}$ na direção de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ e podemos minimizar $J$ alterando $\\mathbf{w}$ na direção de $-\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Isso permite dois métodos bem simples de otimização:\n",
    "\n",
    "**Gradiente Ascendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) + \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "    \n",
    "\n",
    "**Gradiente Descendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "Em ambos os casos o parâmetro $\\eta \\in \\mathbb{R}_{\\geq}$ é chamado de **taxa de aprendizado** (*learning rate*); ele pondera o tamanho de cada atualização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   A seguir são apresentados uma sequência de exercícios que ilustram diferentes aspectos práticos na implementação do algoritmo *gradient descent*  \n",
    "Iremos considerar o problema de regressão linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import numpy as np\n",
    "import time\n",
    "from util import r_squared, randomize_in_place, get_housing_prices_data\n",
    "from plots import simple_step_plot, plot_points_regression, plot_cost_function_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Vamos usar o mesmo dataset de antes, mas agora vamos dividir os dados em treinamento, validação e teste.\n",
    "Essa divisão é comumente realizada na prática: os dados de treinamento são usados na otimização da função custo $J$, os dados de validação são usados para aferir a qualidade da otimização, e os dados de teste são usados para aferir a qualidade da predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dividindo os dados em treinamento, validação e teste\n",
      "\n",
      "train_X shape = (250, 1)\n",
      "\n",
      "train_y shape = (250, 1)\n",
      "\n",
      "valid_X shape = (50, 1)\n",
      "\n",
      "valid_y shape = (50, 1)\n",
      "\n",
      "test_X shape = (50, 1)\n",
      "\n",
      "test_y shape = (50, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = get_housing_prices_data(N=350, verbose=False)\n",
    "randomize_in_place(X, y)\n",
    "\n",
    "train_X = X[0:250]\n",
    "train_y = y[0:250]\n",
    "valid_X = X[250:300]\n",
    "valid_y = y[250:300]\n",
    "test_X = X[300:]\n",
    "test_y = y[300:]\n",
    "\n",
    "print(\"\\nDividindo os dados em treinamento, validação e teste\")\n",
    "print(\"\\ntrain_X shape = {}\".format(train_X.shape))\n",
    "print(\"\\ntrain_y shape = {}\".format(train_y.shape))\n",
    "print(\"\\nvalid_X shape = {}\".format(valid_X.shape))\n",
    "print(\"\\nvalid_y shape = {}\".format(valid_y.shape))\n",
    "print(\"\\ntest_X shape = {}\".format(test_X.shape))\n",
    "print(\"\\ntest_y shape = {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAH+CAYAAAAf9j2+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3X20ZXV95/nPt25VIVdA4FaFGIq6\nRUcmCdodtWoU89QRFYrSCfSKncG+wm1wpBHTgzEzCXatHiZxqpdOsmKKpUBoeSi4t30iGogpxWq0\nk8n0gJaJ8QkNhVJQBKGo4lFQoPjOH/t3vLvO3fucvffZj+e8X2uddc/dZ5+99zl14ffdv9/39/2Z\nuwsAAGCYFU1fAAAA6AaCBgAAkAlBAwAAyISgAQAAZELQAAAAMiFoAAAAmRA0ABPIzDabmccep7Xx\nmE0wsw/EPsOPmr4eoE0IGoAKmdm9fQ1plscNTV83ymVmP4j9+17d9PUARa1s+gIANOI7kv732O97\nW3pMAC1C0ABUa5ukl/Rt+6PY8+9Juqrv9W9mObCZHe3uTxa5KHe/V9IfF3lvnccE0DLuzoMHjxof\nkjz2+G8D9vt4bL/vSPopSddIekDSIUmXhf3eKOlaSX8n6Z8k/UjSM4ru9D8l6fSEY2/uu47TYq99\nILb9R5JeJOk/SvqupB+Hc3xY0lFVHzO8d4Wk90j6dnjvA5I+Iul4SXfEjvv5nP8Ob5H03yX9UNJB\nSZ+RdGr/tfa9J9d33fdvmPY4Ley7SVEAeYekfZKeDt/NA5I+K+k3m/7b5cGDngagG45R1MD9bMJr\n50i6MGH7+vB4q5m9190/VOC8KyT9N0mvjW17qaR3h2s5q4Zj3iDpvNjvPyPpEklvkGQFzi8z+3eS\n4rkF04q+xzdJunPAW6v8rn9d0sUJ238mPN5sZtvd/T0Fjg2UgqAB6IaXhp9fUBQ8rJH0YNj2lKS/\nVjSscVDRnfPxks6Q9Mqwz38yswV335/zvKsUNe6fkvSPks6XdFJ4bbOZ/aK7/0NVxzSzf63DA4aH\nJN2oqJG/UNKROc8tM5uVtD226VlJ10l6VNK5kk4f8Pa83/WCpN2S/g9JR4fX75D057Fj9nI/fhRe\n+5qkA+FcR0v61fCQpEvN7Fp3/0aOjwyUhqAB6I4Puvtl/Rvd/TIzWyFpo6RfkHSspP2SbtFSQ/Yi\nRXeynypw3g+4+/skycz+UlHD1vMaSXmDhjzHfFds+3OSfsXd94T3fU5Rt31e/1bSEfHf3f1j4Zh/\nIukeRT07y+T9rt39s5I+a2b/m5aChn9w92W5H+7+YUkfNrN/LumfS5oJn/kvFQVZq8OumyURNKAR\nBA1AN7ygKKlyGTPbLOnPFHWPD7Ku4LmvjD3/bt9rx1V8zP8x9vxvegGDJLn7X5nZQ5JOyHnu+DF/\nJOkTsWM+YmZ/JeltSW+s8rs2s02KhmJeXvaxgbIQNADd8E+eMFMidLV/RtHd7TBHDN9lmecVJeX1\n/Ljv9SK1XjId08ymJB0V2/6glvuB8gcNx8aeP+LuL/S9/lDSm6r8rs3saEl/pSjZtdRjA2WiuBPQ\nDT9M2X62Dm/ELpV0nLubou7tUR1yd4/97ql7lnxMdz+kaFy/J6lB/ekC538s9nxNGG6ISwtCqvyu\nX6/DP98HJf2Uu1s4/hMjHh8oBUED0G1rYs9d0rXu3msUz23gesr25djzf2lmP+maN7M3K38vgyR9\nJfb8RZL+59gx10h6c8r7Rvmun4s9nx5ybEm6sZe0amZblJJjAdSN4Qmg2+L5ACbpCyFB8Bc0HkHD\n1VqazXCEpP/PzBYlvVjSOwoec4ek/6ClxMIbzOzXtDR7Iq2BHuW73qelXIRzzOwDkh6R9LS7X6nl\neR2fMLNPhfecP/wjAfWgpwHotpsVFT3q+SVJ75f0bxQ1jp3m7p+SdFNs0zpJvy/ptxU1xHfHXuvP\nTUg75r2Sfie2abWi+gjvU1QP4b+nvHWU7zo+a+VoRZ/hjyT9Ybim/1fS7bF9XiHpDyS9U9IuRTM0\ngMYRNAAd5u4/VjS97wZFd64/lnSXokbx3Y1dWLkukPReRVUxn1WUEPlnihrteFLgo1kPGO7uz1Y0\n1fMZSY8rSkR8naT/J+U9o3zX2xVVwLxbhw9VxJ0t6U8Vfb7nFJUYf7+k31TGgAiomh2ejwQA7WJm\nR7r7Mwnbf0nS32qpKuR73H17/34AykPQAKDVzOzPFSUP3ibpXkUVJV+lqJR0bzGwA5JOcffMvQ0A\n8iMREkDbTSmqgrg55fX9ks4hYACqR9AAoO12KFrV89WKpia+SFEOwl2Sdkq6xt0PNHd5wORgeAIA\nAGTC7AkAAJAJwxMJ1qxZ4xs2bGj6MgAAqMVXv/rVR9x97bD9CBoSbNiwQbt37276MgAAqIWZ7c2y\nH8MTAAAgE4IGAACQCUEDAADIhKABAABkQtAAAAAyIWgAAACZEDQAAIBMCBoAAEAmBA0AACATggYA\nAJAJQQMAAMiEoAEAAGRC0AAAADIhaAAAAJkQNAAAgEwIGgAAaLHFRWnDBmnFiujn4mJz17KyuVMD\nAIBBFheliy6Snn46+n3v3uh3SZqbq/966GkAAKCltm5dChh6nn462t4EggYAAFrqvvvyba8aQQMA\nAC21fn2+7VUjaAAAoKW2bZOmpw/fNj0dbW8CQQMAAC01Nyddc400OyuZRT+vuaaZJEiJ2RMAALTa\n3FxzQUI/ehoAAEAmBA0AALREmwo5JWF4AgCAFmhbIack9DQAANACbSvklISgAQCAFmhbIackBA0A\nALRA2wo5JSFoAACgBdpWyCkJQQMAAC3QtkJOSZg9AQBAS7SpkFMSehoAAEAmBA0AACATggYAAJAJ\nQQMAAMiEoAEAgIa0fa2JfsyeAACgAV1Ya6IfPQ0AAJQsSw9CF9aa6EfQAABAiiLDB70ehL17Jfel\nHoT+93ZhrYl+BA0AACTI2vj3y9qD0IW1JvoRNAAAkKDo8EHWHoQurDXRj6ABAIAERYcPsvYgdGGt\niX4EDQAAJCg6fJCnB2FuTrr3Xummm6Lfzzuv3VMvCRoAAEhQdPggbw9C0dyJJhA0AACQYJThg14P\nwgsvRD8HvWdQ7kTbij+Zuzd7BS20adMm3717d9OXAQCYACtWRD0MSaanDw8opqeryXsws6+6+6Zh\n+9HTAABAAWX1AqTlSExNta/4E0EDAAA5lZmHkJY7cehQ8v5NFn8iaAAAIKcyS0Cn5U7Mzibv32Tx\nJ4IGAAByKrsEdFLiZFIPxOrV0lNPNZcYSdAAAEBOdZSA7u+BmJmJhkIOHGhuaiZBAwAAOaXlIWzZ\ncnhy5CWXLE+WzJNAGe+BOOoo6bnnDn+97sRIplwmYMolAGCYxcWowb7vvqiHYcsWaceO5bkOw2Sd\nRpk2NdMsCipGwZRLAAAq1J+HsHNn/oBByt5b0IZVMRsNGszs58zsa7HHE2b2HjM73sx2mdnd4edx\nYX8zsyvMbI+Zfd3MXh071nzY/24zm49t32hm3wjvucLMrInPCgAYb6NMhczy3jasitlo0ODu33X3\nV7r7KyVtlPS0pM9IukzS7e5+iqTbw++SdJakU8LjIklXSZKZHS/pckmvlfQaSZf3Ao2wzztj79tc\nw0cDgInUtrLHdRrljj/Le9uwKmabhifeIOked98r6WxJO8L2HZLOCc/PlnSjR+6QdKyZvVTSmZJ2\nuftBd39U0i5Jm8Nrx7j7HR4lb9wYOxYAoERdWnipCkk9AVnk6S3Is6ZFFdoUNJwr6WPh+Qnu/mB4\n/gNJJ4TnJ0q6P/aefWHboO37ErYDAEpWZsGjLkrqCXjXu6Kpkv16A+VN9BaMohVBg5mtlvQbkj7V\n/1roIah8ioeZXWRmu81s9/79+6s+HQCMnbILHnVRf0/AlVdKjzwiLSwcHkzcdFPUG9NEb8EoWhE0\nKMpV+Dt3fyj8/lAYWlD4+XDY/oCkk2LvWxe2Ddq+LmH7Mu5+jbtvcvdNa9euHfHjAMDkaUN2f1s1\nPaxQlrYEDW/T0tCEJN0qqTcDYl7SLbHt54dZFKdJejwMY9wm6QwzOy4kQJ4h6bbw2hNmdlqYNXF+\n7FgAgBK1Ibsf1Wo8aDCzF0t6k6RPxzZ/QNKbzOxuSW8Mv0vSTknfk7RH0n+WdIkkuftBSe+X9JXw\n+MOwTWGfj4b33CPpc1V+HgCYVG3I7ke1qAiZgIqQAIBJQkVIAABarIs1LQgaAAC162KDWVTSZ+1q\nTQuCBgBArZpqMKsOVPIEB5de2s2aFuQ0JCCnAQCqs2FD1Hj2m52NpiNWodd4xxvqrKtLjnL8I4+U\nDhzIfpwyVqwsImtOA0FDAoIGAKhOlUs8p6k6UEk7fl5VBk6DkAgJAGilJopAVV2tMu9xZma6WdOC\noAEAUKu6ikDFcwxWpLR2ZQUqacdJCw62b+9mTQuCBgBAreooAtWfgHjo0PJ9ygxU0gKhQcFBF0tL\nk9OQgJwGAOiGxcVoxsF990V3+9u2RY1vWo7B1FTUSMf3rfpauiBrTsPKOi4GAICy9c9Y6E1nlNJz\nDF54obpky17vwThjeAIA0Elbt6bXOmDFzWoQNAAAOmnQjAhW3KwGQQMAoJMG9Saw4mY1CBoAAJ00\nrDeh6dkJ47i+BkEDAKCT2tyb0NUFqYYhaAAAdFbTvQlpBiVpDtPmHgqmXAIAULKiZasHTSNtQ0BE\nTwMAYOw0fbdedMrnKD0UdSBoAACMlTbkExSd8ln1wlqjImgAAIyVsu/Wi/RaFE3SbHtRKoIGAMBY\nKfNufZReiyJJmm0vSkXQAAAYG4uL5S6DXXeOQZunkUrMngAAjIler0CZy2A3kWPQ5oWv6GkAAIyF\npF4BKVoOu+jdettzDOpG0AAAGAuDlsMueueeJceg6emddSJoAACMhSp6BYblGLRhemedzN2bvobW\n2bRpk+/evbvpywAA5NBfTVGKegWqTCTcsCEKFPrNzkYzJrrCzL7q7puG7UdPAwCgFlV342edeVDm\ndbS9GFPZmD0BAKhcXWsqDJt5UPZ1rF+f3NMwromS9DQAACpXVr2DUXsJyq670PZiTGUjaAAAVK6M\nbvwykg7Tzrd3b7FhiqqKMbV1RgaJkAlIhASAcpWRMFjlMaTqkyazaiKhk0RIAEBrlNGNX0ZvRdJ1\n9LRlCeo2L49N0AAAqFwZ3fhl1GHoXUeaJmY99A9FpPWEtGFGBkEDAKAWRVZ9jCsr6XBuLgpaktQ9\n6yEpT8OsHdeWhKABAJBJ08l5ab0VUv7rasush6ShCPflgUNbZmQQNAAAhmpLueT+3gqp2HW1ZQnq\ntCEH9+avLQmzJxIwewIADtfGcsmLi9L8fPJS2Hmva3Exuuu/775oGGDbtnoa6bZ8r8yeAACUpm3l\nkns9H0kBg1R//YeiQzdtGSbJiqABADBUFStIjiIpFyAuz3WNOsVxlKCjLcMkWRE0AACGGnRHXFaC\nZJ7jDOpJqLv+w6hBx6izSupE0AAAGGrQzIUyEiTz3q2n9SRMTdVf/6FtQzdVIhEyAYmQAJBNWYl8\neY9TZqnlUY/VlmTGUZAICQCoXFl32XmPU2YuQPxYUtRb0Rte6FLNhzoQNAAACisrQbLIcbLkAmTN\nk5ibW2r8ezMyulbzoQ4EDQCAwsq6y67ibj1vnsQoCY1dSmYcBUEDAKCwsu6yq7hbzxsETFJCY1Ek\nQiYgERIAum/FiqiHoZ9Z1CPQbxwSGosiERIAOqTpxaDGUd48iUlKaCyKoAEAGtaWxaDGTd4gYJIS\nGosiaACAEozSUzBqRcGqdbUXpEgQMCkJjUURNADAiLL0FAxqeNucgNf1XhCCgHKRCJmAREgAeQxL\noBtWcbDNCXhtvjaUpzOJkGZ2rJndbGbfMbO7zOx1Zna8me0ys7vDz+PCvmZmV5jZHjP7upm9Onac\n+bD/3WY2H9u+0cy+Ed5zhZlZE58TwPga1lMwbPihzQl4TfWCdHVIZNw1HjRI2i7p8+7+85J+UdJd\nki6TdLu7nyLp9vC7JJ0l6ZTwuEjSVZJkZsdLulzSayW9RtLlvUAj7PPO2Ps21/CZAEyQYVn6wxre\nNifgZZ2BUGYj3/UhkXHWaNBgZi+R9GuSrpUkd3/W3R+TdLakHWG3HZLOCc/PlnSjR+6QdKyZvVTS\nmZJ2uftBd39U0i5Jm8Nrx7j7HR6Nw9wYOxYAlGJYT0GWhretY+9ZekHKbuTbnhg6yZruaThZ0n5J\n15vZ35vZR83sxZJOcPcHwz4/kHRCeH6ipPtj798Xtg3avi9hOwCUZlhPQZuHH4bJ0gtSdiOfZUiE\n4YtmNB00rJT0aklXufurJP1QS0MRkqTQQ1B5tqaZXWRmu81s9/79+6s+HYAxM6inoM3DD1kM6wUp\nO+9hWM9ME8MXBCmRpoOGfZL2ufud4febFQURD4WhBYWfD4fXH5B0Uuz968K2QdvXJWxfxt2vcfdN\n7r5p7dq1I30oAOhX9fBDUqNWV0NX1kqXPcN6ZuoeviDHYkmjQYO7/0DS/Wb2c2HTGyR9W9Ktknoz\nIOYl3RKe3yrp/DCL4jRJj4dhjNsknWFmx4UEyDMk3RZee8LMTguzJs6PHQsAxkJSo3bBBdKFF9bT\n0BUdfkkLaob1zNQ9o4Mcixh3b/Qh6ZWSdkv6uqS/kHScpBlFsybulvRfJR0f9jVJH5F0j6RvSNoU\nO86FkvaExwWx7ZskfTO858MKtSkGPTZu3OgA0BWzs+5RaDD8MTubfIyFheg1s+jnwkK+a8j7/oUF\n9+npw69tejrbedM+b9pnG5VZ8vnMqjlfEyTt9gxtNsWdElDcCUCXpK3mmCRphcdhxaeqMErRqLqv\ndxIKXHWmuBMAYDR5cgeS9m2i+32UIYa6E0u7PPulbAQNAFCTqhITkxq1Vauk1asP35bW0DVR9XHU\n5Mk661p0ffZLmQgaAKAGVWbgJzVq118vXXddtoau7NkPWXTt7r2txbfqRk5DAnIaAJStzePiTeQ0\n9M67dWvUo7F+fRQwTGpj3DRyGgCgRdqy/HXSEElT3e/cvXcPQQMA1KCJIYB+g4ZI2t6AU5GxHQga\nAKAGbRjDzzJLoo2NMxUZ24OgAQBq0NQQQDwISMqpkJaGSNraOFORsT1IhExAIiSAcZCU4Jikl4zZ\n1mTNtOJVSYWqUAyJkAAw4ZLu0PvFh0iKJGvWMZzRhnwQRAgaAGBMDWrsk4ZI8jbOdQ1ntCEfBBGC\nBgBouaJ382mN/exs8iyJvI1zXbkGdeeDtDEZtC0IGgCgxUa5m88bBGRtnHuN6rDEyv79R2mE65oS\n2tZk0LYgETIBiZAA2mLNGunAgeXbsyYnll11MUtyZfzamqo2WVRbk0GrRiIkAHTc4mJywCBlryRZ\n9h36sOTK/p6MrEMYbRkSaEvlzrYiaACAlhqUG9DUzIFBjWfScEaWRrhNQwLM1BiMoAEAchj1jjjP\n+wc10EVmDpRxNz8ouTKpJyNLI9ym4k3M1BjC3Xn0PTZu3OgA0G9hwX162j26H44e09PR9irePzt7\n+L69x8zM8PPMzrqbRT8XFrKdO+l9o36GpP3N3N/1rqV9zJI/p9ngz1mVLN/DuJG02zO0j4030G18\nEDQASJLWiM/OVvP+IkFK2ntmZgafO8+58jaq73rX8sAgfuxRv1eMLmvQwOyJBMyeAJBk1HLGRd6f\nd/bDoKmQSXrnrnLWQNqxp6akHTui512aYTGOmD0BACUbNUmuyPvzzn7Im+XfO3cVswaG1XM4dCgK\nFqRmFvNCfgQNAJDRqElyo74/SyJjWgAyMzP43GXPGojPiBikl/BYV/EmjIagAQAyGrWc8Sjvzzot\nMS0w2b598LnLnjWQZbGsHmogdAc5DQnIaQDQNllzDhYXpUsvXSoKNTMTBQxZA5Oyqkem5W8kGfdq\ni12QNadhZR0XAwAYTZ4iSfE7/AMHoiBCGh4AzM2VMyywuBgFDYcOLX/N7PBgghoI3cLwBAB0QNEi\nSVIUONRVYbEXuCQFDNPT0sUXk/DYZQQNANABWXIOBuUG1FVhMS1wmZqKAoQrryThscsIGgCgA7Ik\nUQ6b6VBHwmHaOV54gQBhHBA0AEBHDJuWmNQbEVfHokss+DTeCBoAoIWKLC7V642YmVn+Wl0Jhyz4\nNN4IGgCggDJWjBx07KJLRc/NSY88Ii0sNJNwOGotC7QbdRoSUKcBwCBJUxunp6X5eWnnztHrHFS5\nDgSQJGudBoKGBAQNAAZJa9STahAUucsedWEsIC8WrAKAiqTNEOhv6ItOc0xLGlyxoprhkH5VDr2g\n2wgaACCnPDMB8ixT3ZM2C+LQoaUchwsukNasKb9hHyWfAuOPoAEAckpq1M2S9zXL3+D2JxNOTS3f\n57nnokqPZTfsScWZ6ioMhfYjpyEBOQ0Ahulf3GnLFunqq5NzEUZNYMy6+FMZiZLkU0wmEiFHQNAA\noIhBvQ2jNLhpiZdln2fQuZi5Md5IhASAivUnDCYVVZJGr4Y4rNJjWedJOxfFmdBD0AAABSQlDB44\nsHy/Mhrc/hyHmRlp9erRzpM2Q4LiTBiE4YkEDE8AGGbQkEGvXsPsbPECT8P051TkOU9acSqCg8nF\n8AQADDBqLYJBK0b2AoZhSz8vLkbTJs2ix5o12a9j2OJVg1x6KTMkUAxBA4CJU0YtglGXoV5clC68\n8PAhjQMHpLe/XbrkkuzXkdfiYvIwilTP0tnoNoIGABOnjFoEoy5DvXWr9Oyzya9dfXV1xZQGfUaW\nr8YwBA0AJk7aHXWeO+1Rl6EeNrxR1VDBoPMyQwLDEDQAmDhpd9R577RHWYZ6lOGNUfIx0s47M0MS\nJIYjaAAwccquRZCUlDisYd+2bfm0ybi0xn3UfIy0z759e7b3Y7IRNACYOFXXIsjSsM/NSdddJx11\n1PL3DwpgRs3HoA4DRkGdhgTUaQAm1yj1D3rylmLOc07WhkAVqNMAADll7fofNvSQlo+wd2/y/mk1\nF5LOU1Y+BlAEQQMABFm6/rMEFoMa8Kw5CGnn2bKFtSHQHIIGAGMr7yyDLFMxswQWw2o4ZMlBSDvP\nzp3kJKA55DQkIKcB6L4i6ytkyUXImlPQy1MYtD5FUg5C0fcBo+hMToOZ3Wtm3zCzr5nZ7rDteDPb\nZWZ3h5/Hhe1mZleY2R4z+7qZvTp2nPmw/91mNh/bvjEcf094b8qK9wDGSZFZBlmmYmbNKejlKczO\nZttfispHn3deesAw6PxAHRoPGoLXu/srY1HOZZJud/dTJN0efpeksySdEh4XSbpKioIMSZdLeq2k\n10i6vBdohH3eGXvf5uo/DoCmFan6mGU6Yt4aD1n3X1yMykcP6vwldwFNa0vQ0O9sSTvC8x2Szolt\nv9Ejd0g61sxeKulMSbvc/aC7Pyppl6TN4bVj3P0Oj8ZhbowdC8AYKzrLYNjqkXnrHGTdf+vWwQED\nuQtog5VNX4Akl/QFM3NJf+bu10g6wd0fDK//QNIJ4fmJku6PvXdf2DZo+76E7cuY2UWKei+0nv4/\noPO2bUvOaSjjTn1uLl/jnWX/QT0gafUdgLq1oafhV9z91YqGHt5tZr8WfzH0EFSerenu17j7Jnff\ntHbt2qpPB6BiXat8mHavYsaQBNqj8aDB3R8IPx+W9BlFOQkPhaEFhZ8Ph90fkHRS7O3rwrZB29cl\nbAcwAYYNNbRJUu6DmXTxxe2+bkyWRoMGM3uxmR3dey7pDEnflHSrpN4MiHlJt4Tnt0o6P8yiOE3S\n42EY4zZJZ5jZcSEB8gxJt4XXnjCz08KsifNjxwIwgUZZIbJKST0jN90kXXll01cGLGk6p+EESZ8J\nsyBXSvov7v55M/uKpE+a2Tsk7ZX0W2H/nZK2SNoj6WlJF0iSux80s/dL+krY7w/d/WB4fomkGyQd\nKelz4QFgAvXXbuhVWZTacTefN1cCqBvFnRJQ3AkYT3kXkgImRWeKOwFAXYrUbgCwhKABwMRghUhg\nNAQNABrRREJi3mqOAA5H0ACgdlmWl85yjLxBR9dqNwBtQ9AAoHZFFpOKGyXoKFq7oa1TNYE6MXsi\nAbMngGplXV46Td2zIIossw10CbMnALTWqAmJdc2CWFyU1qyR3v720XpGgHFB0ACgdqMmJA5ap2HN\nmnKGEBYXpQsukA4cSN+HqZqYNAQNAGo3akJiUtAhRUMbBw4UT66M27pVeu65wfswVROThpyGBOQ0\nAO23uCjNz0uHDg3er2ieQ1reRQ85DRgn5DQAGGtzc9mSJosOIQzqRZiaImDAZCJoANAKRaY0Zhke\nKDqEsG2btGrV8u2rV0s7dhAwYDIRNABoXNG6C2m5DT1Fqz0uLi7lNESL8EZmZqTrriNgwOQiaADQ\nuLRiT29/++Beh/6EypmZ6DFKtcd4ACNFQcz0tLSwID3yCAEDJhuJkAlIhATq1aakQ5bPxiQiERJA\nZwzLO6izkBLLZwPpCBoANG5YboJUX6PN8tlAOoIGAI2L5yakqaPRXlyUnnpq+XaWzwYiBA0AapU2\ntbK3+uTCwvJeh1WrosZ80HTMUVeh7CVA9peNnpmhJgPQs7LpCwAwOfpXi+xNrZSWGuXez61boyGJ\n44+XnnxyqTFPek+W4w6TNINDko46ioAB6GH2RAJmTwDVKDIzIct7ypjxMOpy3UCXMXsCQKWKDAcU\nmZmQ5T1lzHggARIYjqABQG5FKzgWaZizvKeMBn/U5bqBSUDQACC3tAqOw2opFGmYs7ynjAZ/1OW6\ngUlA0AAgt6LDAUUa5izvKavB783geOGF6CcBA3C4XImQZrZB0nPu/kDf9jMl/bGkl0m6V9J/cveb\nSrvKmpEICQzWVKnl3kJS990XDT1s20bDDpSh9ERIMztB0j2S/mPf9l+QdIukn5P0bUknSrrBzN6Q\n64oBdEYT4/9F8ygAlCfP8MQvSTJJH+vb/h5JqyT9T+6+UdLLJR2U9PulXCGA1mli/L9oHgWA8uQp\n7rROkku6q2/7mZJ2u/ttkuTu95vZDZL+TSlXCKCV5ubqHRpgISmgeUODBjP7kqJgYUPY9AkziydC\nrJe02sy+GNv2M5J+Or7N3U8f/XIBZDVu4//r1yfnUVBHAahPlp6G/zP8/C1JF0v6oKRnwrbXSvp1\nSVdJ+pvYe86Q9DuS/qCMiwQO+gWTAAAgAElEQVQQyRoIlFFWuS16n3nv3mgoJJ67TR0FoF5Dcxrc\n/a/d/a8lfSlselFs208p6oXY0dsWtj8m6Z/6tgEYQZ5EwKrG/0ddFCrvceOfWYo+t1n0PE8eRVXX\nDUyazFMuzew4Sd+T9Lyi3objJL1X0h3u/vq+ff8yHPst5V5uPZhyiTbKM82xinUU+nsvpOhOf9QE\nyEHH7fUw9MsztbOq6wbGSelTLt39UUn/XtLRkv5vSe+T9ICkf9d34vWKkiNvyXPBAAbLkwhYxToK\nab0X8/PJd/BZ7+4H9YqUkfzIrAugPLkqQrr7gqSTFeU3vEnSv3D3f+zb7WhJ75T0yVKuEICkfIFA\nmXUUeo1/0h2/JB06tHy4JG0o5ZJLlgcSgwKDMoIfZl0AJXJ3Hn2PjRs3OtA2Cwvu09PuUTMcPaan\no+1p+8/OuptFP9P2y3vOYY/Z2eiR9JrZ8uufmUk/Tt7PnCTtWmZn838fwLhSVDphaPvI2hNAR2Qt\nqNTrGTjvvOj3m24qvo5CUtf+MHv3pvdK9OdZ9I6d1itSRhEpVq8EypNr7YlJQSIkuqrspL+0hEpJ\nmpqKhib69U+LHMYsCmyqrCkxbjUrgLKVnggJoP3KTvpLyx2YnZV27Fh+Bz8oYOhNlUw6x6irSw5L\numT1SqAcBA3AGCk76W9Q137S0MGgHoaLL65mmICFrID6EDQAY6TsqZbDcgr67+BnZ5OPMzsrXXll\nNYtcMaUSqA85DQnIaUBXNV3IqInzV1HICpg05DQAE6iJJaubPn8VhawAJCNoAMZM/5CBVO+6C3Un\nHTKlEqgPQQMwxookCXZtcaeme1eSdO07BLIipyEBOQ0YF3kWuZKaz4kYB3yH6CJyGgDknoI5KTMR\nquwJmJTvEJOJoAEYY3mTBCdhcaeq6zpMwneIyUXQAHTcoLvmvEmCkzAToeqegEn4DjG5CBqADht2\n19xLEpyZWXrPkUemH6/ITISuJf1V3RPAbA6MM4IGoMOy3jU/88zS8wMH0rvjs8xEiAcJa9ZIF1zQ\nrRLOVfcEtHE2B1AWZk8kYPYEuiJLNcS8MygGSZoZkKTIsevC7AZgOWZPABMg7e74+OOXnpfZHZ/U\ns1HWsetCTwBQXCuCBjObMrO/N7PPht9PNrM7zWyPmX3CzFaH7UeE3/eE1zfEjvG+sP27ZnZmbPvm\nsG2PmV1W92cDqrRtm7Rq1fLtTz65NERQZnd81mCg7Ul/LJUNFNOKoEHSpZLuiv3+QUkfcveXSXpU\n0jvC9ndIejRs/1DYT2Z2qqRzJb1c0mZJV4ZAZErSRySdJelUSW8L+wJjYW5OOuaY5duffXYpr6HM\nxLwswQBJf8D4ajxoMLN1kt4s6aPhd5N0uqSbwy47JJ0Tnp8dfld4/Q1h/7Mlfdzdf+zu35e0R9Jr\nwmOPu3/P3Z+V9PGwLzA2Dh5M3t7rFcg7g2KQpABk9ero2HT1A+Ov8aBB0p9K+j1JvUVsZyQ95u7P\nh9/3SToxPD9R0v2SFF5/POz/k+1970nbvoyZXWRmu81s9/79+0f9TEBtsg4/ZJ1BMUhSPsB110mP\nPEJXPzAJGg0azOwtkh529682eR2S5O7XuPsmd9+0du3api8HyCzL8EOZBY3IBwAmV9M9Db8s6TfM\n7F5FQwenS9ou6VgzWxn2WSfpgfD8AUknSVJ4/SWSDsS3970nbTswNrLMBhh1BkXXCjgBqEajQYO7\nv8/d17n7BkWJjF909zlJX5L01rDbvKRbwvNbw+8Kr3/Ro0ITt0o6N8yuOFnSKZK+LOkrkk4JszFW\nh3PcWsNHA2o17O5/lBkUVa/VAKA7mu5pSPP7kt5rZnsU5SxcG7ZfK2kmbH+vpMskyd2/JemTkr4t\n6fOS3u3uh0Lew29Luk3R7IxPhn2B1ivz7n6UGRRlDG3QUwGMCXfn0ffYuHGjA6NaWHCfnXU3i34u\nLOR77/S0e3RvHz2mp/Mdo6zrMTv8OnoPs2znnJlZ/t5RPwuAckna7RnaR8pIJ6CMNEY1aqnitNLP\nU1PSjh31Jh8WLUM9rOR0m0tNA5OGMtJAg0bt0k9LUDx0qJx8gjzDBUWHNoaVnG5zqWkAyQgagAqM\nOlthUIJiPPgokiuQN7Gx6FoNwz5r20tNA1iO4YkEDE9gVKOuLDmsa99MuummYkMgZa56OUjaeSRW\nlQTahuEJoEGjrvfQu7ufmkp+ff364kMgZa56OUjSdyBFJacJGIBuImgAKlDG8stzc1HSY1rwUbTx\nH7Scdv9QxyhTJZO+g4WFqOQ0AQPQTQQNQEX6Cy5J+RvgQcFH0YJNST0Aq1ZFy2nH8xwuvFC64ILR\nijpRchoYLwQNQA1GqaqY1vAWHQJJCkSOOSZaTjvu2Wel5547fFvR9SoAjAcSIROQCImyVZV8uLgY\nNeL33Rf1MGzbVuxufsWKKJjJwiwKYACMj6yJkCuH7QBgdFUlH87NldPlv359+kyHpH0BTCaGJ4AK\n9RIJ0+7i29IAJw11rF4d5TrE5ZkBAmD8EDQAFYnnMSRpUwOclOdw3XXS9dePNgMEwHghaABKkDQ1\ncVAZ5TY2wEkJl2lJmEmfl5UsgfFHImQCEiGRR9riVIOqObYlkbBIImXS5121Kvpc8RkYVH0EuoOK\nkOisrt2xplVmHFTNsQ2GTQNN+3dI+rzPPbd8yibTM4Hxw+wJtEr/XWyvIZPae8c6aEXK/h6HNuUx\nDCtDnfbvkGfGBytZAuOFnga0yqhLSjchreegl7dQVyJh3h6aQdNAB/075OkpaUuvCoByEDSgVepa\nTKlMgyoz1lVGOW2o4ZJL0gOJQWWoB/07pJWhXr368G1t6lUBUA6CBrRK0fUUmlTG4lSjSusZuPrq\n9JyFQcHOoH+HpM97/fXRFE2mZwLjjdkTCZg90Zy0mQg0QIPlKQMdL12dNnuCfwdgsjB7Ap3Uhrv2\nLNo2wyNPT0x86CFt+KQr/w4A6kVPQwJ6GjBIG+/Ck64pzaiLZAEYP/Q0ACPKU6eg6Rkec3PS/HzU\nKzAIyYkARkHQgLFS1rDBoMJHWWZ4NDF8sXNncl7D1BRDDADKwfBEAoYnuqnMYYMNG5IXmpqdjX6m\nvXbvvc0NX6QlQ7apbDWAdmJ4AhOnzGGDvHUK4t3+Ra6jjJ6JLk5XBdAtBA0YG1kLQ2VpoPPWKYj3\nIqQthZ12fcPWgMhqWDADAKMiaMDYyHKnnbWBHtYAD1oyOi0Z0T05SCmrh4RpkgCqRk5DAnIauilL\nLsGgXIX+aYhFlo1OO35c/zWRiwCgaeQ0YOJkudPOs7ZF1nUj4sMdwwIGaXkvArkIALqCoAFjZVhD\nX3YD3T/ckVU8SCEXAUBXEDSgMU3UMkhqoCXpqaeKnT8pHyGLeJBCLgKAriBoQCPKmjGQV6+Bnpk5\nfPuBA8XOX2TJ7qRehF4PyU03Rb+fd1471rQAgDiCBjSiyVLMc3PSUUct317k/FmHNbL0IjQVSAFA\nVgQNaESehMQ2nz9tuCOuNzNjWEJl3YFU21bqBNB+BA1oRBUzBvI0gmWdP56PIC2v0ZAnobHOQIpe\nDQBFEDSgEWXPGMjbCJZ5/l4+gnuUk1A0obHOqZdtXKkTQPtR3CkBxZ3qUaR4Upo8RZuqOH8Z6lzo\nioJSAOKyFnciaEhA0NA949II1hXIFAmyAIwvKkJiooxLVcWsVShHRUEpAEUQNGAs0AjmQ0EpAEWs\nbPoCgDL0Grs25Si03dwc3w+AfOhpwNgY1LVPTQIAGB1BA8Ze2TUJCEAATCqCBoy9MmsS1FUUicAE\nQBsRNGDsDau0mKeBTgtA5ufLa9ip1gigrQga0Kg67qgHTcfM20CnBSCHDpXXsFOtEUBbETSgMXXd\nUQ+ajpm3gR5U9yFrwz4sUGp6MS8ASEPQgMbUdUc9qCZB1ga619Dv3bt8UapB7+uXJVAal0JVAMYP\nZaQTUEa6Hm0o/ZylnHLSmhBphpVhLnq+qtagAACJMtLogDbcUacNXWzZsjSEMD+fHDAUWQY7S88G\n1RoBtBVBAxrThtLPSQ30/Ly0Y8fSEMKhQ8nvdc/fsGcNlOpagwIA8mg0aDCzF5nZl83sH8zsW2b2\nB2H7yWZ2p5ntMbNPmNnqsP2I8Pue8PqG2LHeF7Z/18zOjG3fHLbtMbPL6v6MSNeWO+r+BnrnznxD\nEXka9jYESgBQVNM9DT+WdLq7/6KkV0rabGanSfqgpA+5+8skPSrpHWH/d0h6NGz/UNhPZnaqpHMl\nvVzSZklXmtmUmU1J+oiksySdKultYV+0RBvvqLPMUija0LclUAKAIhoNGjzyVPh1VXi4pNMl3Ry2\n75B0Tnh+dvhd4fU3mJmF7R939x+7+/cl7ZH0mvDY4+7fc/dnJX087IsxUnath7QhhKmpchr6NgZK\nAJBF0z0NCj0CX5P0sKRdku6R9Ji7Px922SfpxPD8REn3S1J4/XFJM/Htfe9J244xUUWth7QhhB07\naOgBTLbGgwZ3P+Tur5S0TlHPwM83cR1mdpGZ7Taz3fv372/iElBAFbUeGEIAgGSNBw097v6YpC9J\nep2kY81sZXhpnaQHwvMHJJ0kSeH1l0g6EN/e95607Unnv8bdN7n7prVr15bymZBf3qGGqqon1jGE\nwKJUALqm6dkTa83s2PD8SElvknSXouDhrWG3eUm3hOe3ht8VXv+iR9WpbpV0bphdcbKkUyR9WdJX\nJJ0SZmOsVpQseWv1nwxpBjWURYYa0vIPjj++zKsuH4tSAeiipnsaXirpS2b2dUUN/C53/6yk35f0\nXjPboyhn4dqw/7WSZsL290q6TJLc/VuSPinp25I+L+ndYdjjeUm/Lek2RcHIJ8O+aMCwhrLIUMO2\nbdKqVcu3P/lkuxtgFqUC0EWUkU5AGelqDCuhXLSs9Jo10oED6cdtozaU0AaAHspIo3WG5R8ULSt9\n8GC+87VBG0poA0BeBA2ozbCGsmi1xC42wFSGBNBFBA2ozbCGcthUx7Qkyi42wEzrBNBFK4fvApSj\n1yBeeulSDsKRRy7fJ6nh7F8uupdEGT/u1q3RkMT69VHA0PYGOO2zAkBb0dOA2j3zzNLzAweyTTUc\nNtuA0swAUD2CBtSq6FTDqoo4AQCyI2hArYo2/l0t4gQA44SgAbUqOtNh2zZp9erl2594ot1FnABg\nnBA0TJgm1ztYXJSeemr59iwzHebmpKOPXr79uefqq6LIWhEAJh1BwwRpcr2D3rn7KzfOzGSfali0\niFMZjT1rRQAAZaQTjWsZ6WFlnNt+7iLH6J+qKUU9G3lrIjT53QFA1SgjjWWqmIGQ9S6+jHMXKeJU\n1sJQzN4AAIKGThm1m73scst5uuzLOHeRKoplNfZdLFUNAGUjaOiIpAb6vPOixjNrADFqueX+oOXS\nS7PfxZdV6jlvEaeyGvsulqoGgNK5O4++x8aNG71tZmfdo3Ah+TE97b6wMPw4CwvRscyin1ne03vf\n9PTga+g9zMo99yiSrjvrd5V0rLqvHwDqIGm3Z2gfSYRM0MZEyBUroiZvkKxJeYuL+ddpSEsEHOU6\nisp7/UU+LwBMkqyJkAQNCdoYNGRptM2ibvtBis4myBK09MzMSNu3V9MwlzUbAgCwhNkTYyZpTL1f\nlnH6orMJ0o49MxM94rIuQlVEWbMhAAD5ETR0RHzmgBT1KsRlTcorOpsgLRFw+3bpqKOW719VQ571\n+qneCADlI2jokN7MAXfpppvyTT3sKTqbYNB0xzprGGS5fqo3AkA1CBpaoMhdcd6phz2jTB1MO2ed\nNQyyXD9DGABQDYKGhtV9V1ykQFLvOtMCmyprGPSfVzp8mGZqaikg6F0T1RsBoBrMnkhQ5+yJLqxp\nkGXGQv+0xi1bpJ07R5vmOOi8UvprW7e2/zsFgDZhyuUI6gwa0qYyZpk+WZe8gU0di0RJ6a9t25b/\n/NRyADDJCBpGQE/D4fIGNmV9pkHnlQZfU54ggNoPACYddRo6ogtrGuRNdEzLHdi7N9/0x0HnHXZN\neRJFSZwEgGwIGhpWNDGxTnkDm0GzJvIkeg46b5nBFomTAJANQUMLFJ0+WZZhUz7zBjbDqldmvYsf\ndN4ygy2WvQaAbMhpSNDGtSeqUtV4fi+nIG29jDYlepLTAGDSkdOATKoaz+/1nvRmOvQb9S6+zDLR\nXRgiAoA2IGiYcFWP51eR6FlFQaymh4gAoAsIGiZc1eP5VdzFM9sBAJpB0DDh0noCtmwpt/u/zLt4\nZjsAQDMIGsZE0TH+pJ6A+Xlpx47Du//POy96vQ3LTDPbAQCaQdDQgDwNfJZ9Rx3jj/cEbNsWBRH9\n3f+9STZtWGa6CwWxAGAcMeUyQZVTLpOm90nSzIy0ffvhXfdZpwKWVbY57dqSNF3mmrUiAKA8rD0x\ngiqDhrQGXloeEGQNBspa9GrQtY16bABAe1GnoWV6wwyDGuX+GQBZE/7KGuPPk0hYZv5AmTUXAADV\nIWioQTznYJh4w501GChrjD9rIFBm/kAVNRcAANUgaKhBUl2BNPGGe1gw0LtDP+886cgjo7yIUWoh\npJ3vXe9KrrNQRg9BWs2F+XkCBwBoHXfn0ffYuHGjl8nMPbqPHvyYnnZfWIjes7DgPjsbbZ+ain7O\nzh7++vR0+vuL6p3X7PDzJe1XxvkHfTdlfB4AwHCSdnuG9pFEyARlJ0Km5TLMzEhHHbV8BkCWWRNl\nzZgoqqzzD8vzaHqWBgBMAhIhWySt23/79uRKiVnKJJdRFXGU4YWyqjIOW0abKo8A0B4EDTXIu/5C\nlgZ51BkToyYgljVjo/fdTE2VczwAQHUIGmqSZ/2FLA3yqDMmRl30qcyqjHNzUdlqqjwCQLsRNLRQ\nlgZ51NUjRx1eKHv1yipWwwQAlItEyARVVoTMquoyyWUmUlLSGQC6jUTIFimScFj2ctL9yhpeoDgT\nAEwOgoaKtbVRLWs4YFBuBOWhAWC8MDyRoMzhiabrKVQtbbEsKeq5GLZCJwCgeQxPtESehMMu3pmn\nzfSYmhptdgYAoH0IGiqWtZ5BW4cxhknLjTh0KHl/ijUBQHcRNFQsa8LhqHUTmpKWGzE7m7w/xZoA\noLsaDRrM7CQz+5KZfdvMvmVml4btx5vZLjO7O/w8Lmw3M7vCzPaY2dfN7NWxY82H/e82s/nY9o1m\n9o3wnivMzOr8jFkTDssqy9yEpJkeZRZ/AgC0Q9M9Dc9L+l13P1XSaZLebWanSrpM0u3ufoqk28Pv\nknSWpFPC4yJJV0lRkCHpckmvlfQaSZf3Ao2wzztj79tcw+c6TJbpk2WVZW4LijUBwPhpNGhw9wfd\n/e/C8ycl3SXpRElnS9oRdtsh6Zzw/GxJN4aVPO+QdKyZvVTSmZJ2uftBd39U0i5Jm8Nrx7j7HWHp\nzxtjx2qVtIWbHnmk/XkNaaquNQEAqFfTPQ0/YWYbJL1K0p2STnD3B8NLP5B0Qnh+oqT7Y2/bF7YN\n2r4vYXvrzM1J8/PLt//wh9KFF3Y3cAAAjI9WBA1mdpSkP5f0Hnd/Iv5a6CGovJiEmV1kZrvNbPf+\n/furPl3i9MqdO5P3ffbZ6hIiuzjNEwDQjMaDBjNbpShgWHT3T4fND4WhBYWfD4ftD0g6Kfb2dWHb\noO3rErYv4+7XuPsmd9+0du3a0T7UEGnTK5OKQPXEEyLLaui7Os0TANCMpmdPmKRrJd3l7n8Se+lW\nSb3O+nlJt8S2nx9mUZwm6fEwjHGbpDPM7LiQAHmGpNvCa0+Y2WnhXOfHjtWYtOmVU1Pp7+klRJbZ\n0Hd1micAoBlN9zT8sqTzJJ1uZl8Ljy2SPiDpTWZ2t6Q3ht8laaek70naI+k/S7pEktz9oKT3S/pK\nePxh2Kawz0fDe+6R9Lk6PtggadMoDx2SVq1avn316qWpilka+qw9EV2e5gkAqB9rTySoemnsQetR\nbNsmXXqpdOBAtG1mRtq+fWnmQdpaD2bRLIVeT0SWNR/GfV0MAEA2rD3RYoMKH83NRdMs3aPHI48c\n3tgPq+eQZ8iBAkwAgDwIGhowSuGjYQ19niEHCjABAPJgeCJB1cMTo1pcjHoO7rsv6mHo9VBIDDkA\nAPJjeGKMDaq0yJADAKAqBA1jhiEHAEBVCBparGgRJ9Z8AABUYWXTF4Bk/VMne0WcJIIAAEAz6Glo\nKao1AgDahqChpajWCABoG4KGlhpWxCkPVrIEAJSBoKFiRRvstKmTW7YMPl7/+S65hJUsAQDloLhT\ngrKKO+VZByLt/fEiTlu2SDt2pB8v6XxmyWtVUOwJANCTtbgTQUOCsoKGsqszDjte2utJegtcAQBA\nRcgWKDuZcdjx8hy3SG4EAGCyETRUqMxkxizHS3vd7PDfKSsNACiCoKFCZa8DMex4aa9ffDFlpQEA\no6MiZIV6DXPaipRlH6/s8wEAEEdPQ8X614GQRquZMGxdibLPBwBAD0FDjXpTIuuqmVD3+eLnJVAB\ngPHDlMsEZU257Ff2FMy2nU8avTYFAKB+1GkYQVVBw4oVyYWWqqqZUPf5pGYCFQDAaKjT0EJlT8Fs\n2/kkFtoCgHFG0FCjsqdgtu18UjOBCgCgHgQNFepPCJSisf26aibMzdV7PqmZQAUAUA9yGhKUkdMw\nyQmB/QttUSsCANqNRMgRlBE0kBAIAOgKEiEbRkIgAGDcEDRUhIRAAMC4IWioCAmBAIBxQ9BQkSZm\nLgAAUCVWuazQ3BxBAgBgfNDTAAAAMiFoAAAAmRA0AACATAgaAABAJgQNAAAgE4IGAACQCUFDR/Sv\nmLm42PQVAQAmDXUaOqB/xcy9e6PfJepAAADqQ09DB2zdevgS21L0+9atzVwPAGAyETR0ACtmAgDa\ngKChA1gxEwDQBgQNHcCKmQCANiBo6ABWzAQAtAGzJzqCFTMBAE2jpwEAAGRC0AAAADIhaAAAAJkQ\nNAAAgEwIGgAAQCYEDQAAIBOCBgAAkAlBAwAAyKTRoMHMrjOzh83sm7Ftx5vZLjO7O/w8Lmw3M7vC\nzPaY2dfN7NWx98yH/e82s/nY9o1m9o3wnivMzOr9hAAAjI+mexpukLS5b9tlkm5391Mk3R5+l6Sz\nJJ0SHhdJukqKggxJl0t6raTXSLq8F2iEfd4Ze1//uQAAQEaNBg3u/jeSDvZtPlvSjvB8h6RzYttv\n9Mgdko41s5dKOlPSLnc/6O6PStolaXN47Rh3v8PdXdKNsWMBAICcmu5pSHKCuz8Ynv9A0gnh+YmS\n7o/tty9sG7R9X8J2AABQQBuDhp8IPQRex7nM7CIz221mu/fv31/HKQEA6JQ2Bg0PhaEFhZ8Ph+0P\nSDoptt+6sG3Q9nUJ2xO5+zXuvsndN61du3bkDwEAwLhpY9Bwq6TeDIh5SbfEtp8fZlGcJunxMIxx\nm6QzzOy4kAB5hqTbwmtPmNlpYdbE+bFjAQCAnCwaAWjo5GYfk/TrktZIekjRLIi/kPRJSesl7ZX0\nW+5+MDT8H1Y0A+JpSRe4++5wnAsl/Ydw2G3ufn3YvknRDI0jJX1O0r/3DB/YzPaHc0+CNZIeafoi\nWo7vKBu+p+H4jobjO8qm7O9p1t2HdrM3GjSgeWa22903NX0dbcZ3lA3f03B8R8PxHWXT1PfUxuEJ\nAADQQgQNAAAgE4IGXNP0BXQA31E2fE/D8R0Nx3eUTSPfEzkNAAAgE3oaAABAJgQNY8zMTjKzL5nZ\nt83sW2Z2adieeyXRcWdmU2b292b22fD7yWZ2Z/guPmFmq8P2I8Lve8LrG5q87jqZ2bFmdrOZfcfM\n7jKz1/G3dDgz+53w39o3zexjZvYi/paqX9F4HKR8R38U/nv7upl9xsyOjb32vvAdfdfMzoxt3xy2\n7TGzy/rPMyqChvH2vKTfdfdTJZ0m6d1mdqpyriQ6IS6VdFfs9w9K+pC7v0zSo5LeEba/Q9KjYfuH\nwn6TYrukz7v7z0v6RUXfF39LgZmdKOl/lbTJ3V8haUrSueJvSap+ReNxcIOWf0e7JL3C3f+FpH+U\n9D5JCv8fP1fSy8N7rgw3PlOSPqLoOzxV0tvCvqUhaBhj7v6gu/9deP6kov/Jn6j8K4mONTNbJ+nN\nkj4afjdJp0u6OezS/x31vrubJb0h7D/WzOwlkn5N0rWS5O7Puvtj4m+p30pJR5rZSknTkh4Uf0uV\nrmhc/dXXI+k7cvcvuPvz4dc7tLQ0wtmSPu7uP3b370vaoyiQeo2kPe7+PXd/VtLHw76lIWiYEKHr\n81WS7lT+lUTH3Z9K+j1JL4TfZyQ9FvuPNf49/OQ7Cq8/HvYfdydL2i/p+jCM81Eze7H4W/oJd39A\n0h9Luk9RsPC4pK+Kv6U0Za1oPCkuVFTZWGrwOyJomABmdpSkP5f0Hnd/Iv5anSuJtpGZvUXSw+7+\n1aavpeVWSnq1pKvc/VWSfqil7mRJ/C2FrvKzFQVYPyPpxRqjO+EqTfrfzjBmtlXRcPNi09dC0DDm\nzGyVooBh0d0/HTbnXUl0nP2ypN8ws3sVdeWdrmjs/tjQxSwd/j385DsKr79E0oE6L7gh+yTtc/c7\nw+83Kwoi+Fta8kZJ33f3/e7+nKRPK/r74m8pWVkrGo81M/u3kt4iaS62dlJj3xFBwxgL46PXSrrL\n3f8k9lLelUTHlru/z93XufsGRYlFX3T3OUlfkvTWsFv/d9T77t4a9h/7OyR3/4Gk+83s58KmN0j6\ntvhbirtP0mlmNh3+2+t9R/wtJStlReO6L7pOZrZZ0dDpb7j707GXbpV0bpiBc7KipNEvS/qKpFPC\njJ3Viv6fdmupF+XuPMb0IelXFHX5fV3S18Jji6Jx09sl3S3pv0o6PuxvijJv75H0DUVZ4I1/jhq/\nr1+X9Nnw/J+F/wj3SPqUpCPC9heF3/eE1/9Z09dd4/fzSkm7w9/TX0g6jr+lZd/RH0j6jqRvSrpJ\n0hH8LbkkfUxRnsdzinqt3lHkb0fRuP6e8Lig6c9Vw3e0R1GOQu//31fH9t8avqPvSjortn2LopkW\n90jaWvZ1UhESAABkwkEE/DEAAAIRSURBVPAEAADIhKABAABkQtAAAAAyIWgAAACZEDQAAIBMCBoA\ntJ6ZvdHM7jCze8zsATP7WzP71aavC5g0BA0AuuAxSf+Lu/+spFlFRZN2jtkqh0DrETQAaD133+3u\n3wzPn1dUFOgoTdaCRUDjKO4EoFPMbFpRudzHJP2K8z8xoDb0NACojJltMDM3sxvM7GfN7GYzO2Bm\nT5rZF8zsFWG/tWZ2jZk9aGY/MrOvmNnrE463UlHp5ZdIehsBA1AvehoAVMbMNkj6vqS/lvQKSXcp\nWmdhg6R/JemgpNdJ+rykJ8J+xytaaOcFSf+Du98XjrVa0icVra75Jnf/bn2fBIBETwOAevxLSR9y\n9191999199+UdLmiRYvulLRL0kZ3f4+7n69osZ4jJP2OJJnZiyX9paSTJf0SAQPQDHoaAFQm1tNw\nr6SXufuh2GvrJe2V9LSkn3b3J2OvTUn6kaS/dffXm9lWSf+XpH+S9EzsFL/n7p+u+GMACAgaAFQm\nFjT8hbv/q77XVipaBvhr7v6qhPfuk/SMu59Sw6UCyIDhCQB1eLx/Q5g6mfha8LykVZVdEYDcCBoA\nAEAmBA0AACATggYAAJAJQQMAAMiEoAEAAGTClEsAAJAJPQ0AACATggYAAJAJQQMAAMiEoAEAAGRC\n0AAAADIhaAAAAJkQNAAAgEwIGgAAQCYEDQAAIBOCBgAAkMn/DxiLIyIPlsNBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69d842f668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_points_regression(train_X,\n",
    "                       train_y,\n",
    "                       title='Training data',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para minimizar a função de custo vamos ter que colocar os dados em uma outra escala\n",
    "\n",
    "Um modo de fazer isso é o chamado [standard/z score](https://en.wikipedia.org/wiki/Standard_score).\n",
    "Aplicamos a seguinte transformação: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^{\\top}_{i} \\leftarrow \\frac{\\mathbf{X}^{\\top}_{i} - \\mu_{i}}{\\sigma_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\mathbf{X}^{T}_{i} \\in \\mathbb{R}^{N}$ ($i = 1, \\dots, d$) é um vetor de features da design matrix $\\mathbf{X}$, $\\mu_{i}$ é a média de tal vetor, e $\\sigma_{i}$ seu desvio padrão.\n",
    "\n",
    "A importância de se fazer essa transformação é discutida mais ao final deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 1)** \n",
    "Use a biblioteca numpy para implementar a função que altera os dados conforme a equação acima (essa função deve funcionar para uma design matrix $\\mathbf{X}$ com um número arbitrário de features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Returns standardized version of the ndarray 'X'.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: standardized array\n",
    "    :rtype: np.ndarray(shape=(N, d))\n",
    "    \"\"\"\n",
    "    X_out = (X.transpose() - np.mean(X))/np.std(X);\n",
    "    # YOUR CODE HERE:\n",
    "    # raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    toy_X = np.array([[1100.3, 2.4, 34.34],\n",
    "                      [2300.3, 1.4, 442.23]])\n",
    "    toy_y = np.array([[1000.2], [2000.5]])\n",
    "    toy_X_norm = standardize(toy_X)\n",
    "    toy_y_norm = standardize(toy_y)\n",
    "    xmean, xstd = np.mean(toy_X_norm), np.std(toy_X_norm)\n",
    "    ymean, ystd = np.mean(toy_y_norm), np.std(toy_y_norm)\n",
    "    assert -1 <= xmean < 0\n",
    "    assert 0 <= ymean < 1\n",
    "    assert 0.9 <= xstd <= 1\n",
    "    assert 0.9 <= ystd <= 1\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados originais\n",
      "\n",
      "X:\n",
      "mean 636.5652465820312, std 323.76, max 1200.0, min 93.1805191040039\n",
      "\n",
      "y:\n",
      "mean 43799.578125, std 16026.71, max 76620.0078125, min 12718.060546875\n",
      "\n",
      "Dados normalizados\n",
      "\n",
      "X:\n",
      "mean 7.915496524901755e-08, std 1.00, max 1.740271806716919, min -1.6783435344696045\n",
      "\n",
      "y:\n",
      "mean 1.0299682884351569e-07, std 1.00, max 2.0478577613830566, min -1.939357042312622\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-30f8dc8012a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                            \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training data (normalized)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                            \u001b[0mxlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"m\\u00b2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                            ylabel='$')\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/ime/eps/mac5832_linreg/plots.py\u001b[0m in \u001b[0;36mplot_points_regression\u001b[0;34m(x, y, title, xlabel, ylabel, prediction, legend, r_squared, position)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \"\"\"\n\u001b[1;32m     38\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mline1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Real data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mline2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Predicted data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 1)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAHVCAYAAADYaHMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69d6323710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    train_X_norm = standardize(train_X)\n",
    "    train_y_norm = standardize(train_y)\n",
    "\n",
    "\n",
    "    xmean, xstd = np.mean(train_X), np.std(train_X)\n",
    "    xmax, xmin = np.max(train_X), np.min(train_X)\n",
    "    ymean, ystd = np.mean(train_y), np.std(train_y)\n",
    "    ymax, ymin = np.max(train_y), np.min(train_y)\n",
    "\n",
    "    print(\"Dados originais\\n\")\n",
    "    print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                           xstd,\n",
    "                                                           xmax,\n",
    "                                                           xmin))\n",
    "    print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                             ystd,\n",
    "                                                             ymax,\n",
    "                                                             ymin))\n",
    "\n",
    "\n",
    "    xmean, xstd = np.mean(train_X_norm), np.std(train_X_norm)\n",
    "    xmax, xmin = np.max(train_X_norm), np.min(train_X_norm)\n",
    "    ymean, ystd = np.mean(train_y_norm), np.std(train_y_norm)\n",
    "    ymax, ymin = np.max(train_y_norm), np.min(train_y_norm)\n",
    "\n",
    "    print(\"Dados normalizados\\n\")\n",
    "    print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                           xstd,\n",
    "                                                           xmax,\n",
    "                                                           xmin))\n",
    "    print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                             ystd,\n",
    "                                                             ymax,\n",
    "                                                             ymin))\n",
    "    plot_points_regression(train_X_norm,\n",
    "                           train_y_norm,\n",
    "                           title='Training data (normalized)',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$')\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Adicionando uma componente com apenas 1s como uma nova feature\n",
    "Conforme já vimos, adicionar uma componente (coordenada artificial) constante 1 é conveniente. Isto é, em vez de $\\mathbf{x} \\in \\mathbb{R}^d$ é conveniente considerarmos $(1,\\mathbf{x}) \\in \\mathbb{R}^{d+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_ones(X):\n",
    "    \"\"\"\n",
    "    Returns the ndarray 'X' with the extra\n",
    "    feature column containing only 1s.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: output array\n",
    "    :rtype: np.ndarray(shape=(N, d+1))\n",
    "    \"\"\"\n",
    "    return np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "try:\n",
    "    train_X_1 = add_feature_ones(train_X_norm)\n",
    "    print(\"\\ntrain_X shape = {}\".format(train_X_1.shape))\n",
    "\n",
    "except NameError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Criando a predição da regressão linear e plotando uma predição arbitrária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_prediction(X, w):\n",
    "    \"\"\"\n",
    "    Calculates the linear regression prediction.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: prediction\n",
    "    :rtype: np.array(shape=(N, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    return X.dot(w)\n",
    "\n",
    "try:\n",
    "    w = np.array([[1.2], [2.3]])\n",
    "    prediction = linear_regression_prediction(train_X_1, w)\n",
    "\n",
    "    plot_points_regression(train_X_norm,\n",
    "                           train_y_norm,\n",
    "                           title='Training data (normalized)',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$',\n",
    "                           prediction=prediction,\n",
    "                           legend=True)\n",
    "except NameError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computando a função de custo\n",
    "\n",
    "Usando o erro quadrárico médio, a função de custo $J(\\mathbf{w})$ para a tarefa de regressão linear pode ser escrita de dois modos. A forma iterativa:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_{i} - y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "e a forma vetorial:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})^{T}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 2)**  \n",
    "Use a biblioteca numpy para implementar a função de custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates  mean square error cost.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: cost\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    toy_w = np.array([[1], [1], [2]])\n",
    "    toy_X = np.array([[2, 3, 1],\n",
    "                      [5, 1, 2]])\n",
    "    toy_y = np.array([[1], [1]])\n",
    "    assert compute_cost(toy_X, toy_y, toy_w) == 58.5\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos olhar a superficie de custo e ver onde se situa um valor $J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    initial_w = np.array([[15], [-35.3]])\n",
    "    initial_J = compute_cost(train_X_1, train_y_norm, initial_w)\n",
    "\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=\"Optimization landscape\",\n",
    "                             weights_list=[initial_w.flatten()],\n",
    "                             cost_list=[initial_J])\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando os gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É fácil calcular a derivada parcial de $J(\\mathbf{w})$ com relação a cada entrada $j$ de $\\mathbf{w}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{j}} = \\frac{2}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i) \\mathbf{x}_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Lembre que o gradiente de $J(\\mathbf{w})$ com relação a $\\mathbf{w}$ é:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = \\begin{bmatrix}\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{1}} \\dots \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{m}} \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 3)**  \n",
    "Use a biblioteca numpy para calcular $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wgrad(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates gradient of J(w) with respect to w.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: gradient\n",
    "    :rtype: np.array(shape=(d, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(X, y, w, h=1e-4):\n",
    "    \"\"\"\n",
    "    Check gradients for linear regression.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param h: small variation\n",
    "    :type h: float\n",
    "    :return: gradient test\n",
    "    :rtype: boolean\n",
    "    \"\"\"\n",
    "    Jw = compute_cost(X, y, w)\n",
    "    grad = compute_wgrad(X, y, w)\n",
    "    passing = True\n",
    "    d = w.shape[0]\n",
    "    for i in range(d):\n",
    "        w_plus_h = np.array(w, copy=True)\n",
    "        w_plus_h[i] = w_plus_h[i] + h\n",
    "        Jw_plus_h = compute_cost(X, y, w_plus_h)\n",
    "        w_minus_h = np.array(w, copy=True)\n",
    "        w_minus_h[i] = w_minus_h[i] - h\n",
    "        Jw_minus_h = compute_cost(X, y, w_minus_h)\n",
    "        numgrad_i = (Jw_plus_h - Jw_minus_h) / (2 * h)\n",
    "        reldiff = abs(numgrad_i - grad[i]) / max(1, abs(numgrad_i), abs(grad[i]))\n",
    "        if reldiff > 1e-5:\n",
    "            passing = False\n",
    "            msg = \"\"\"\n",
    "            Seu gradiente = {0}\n",
    "            Gradiente numérico = {1}\"\"\".format(grad[i], numgrad_i)\n",
    "            print(\"            \" + str(i) + \": \" + msg)\n",
    "            print(\"            Jw = {}\".format(Jw))\n",
    "            print(\"            Jw_plus_h = {}\".format(Jw_plus_h))\n",
    "            print(\"            Jw_minus_h = {}\\n\".format(Jw_minus_h))\n",
    "\n",
    "    if passing:\n",
    "        print(\"Gradiente passando!\")\n",
    "    \n",
    "    return passing \n",
    "\n",
    "try:\n",
    "    toy_w1 = np.array([[1.], [2.], [1.], [2.]])\n",
    "    toy_X1 = np.array([[2., 3., 1., 2.],\n",
    "                      [5., 1., 1., 2.]])\n",
    "    toy_y1 = np.array([[1.], [-1.]])\n",
    "    toy_w2 = np.array([[-100.22], [20002.1], [102.5]])\n",
    "    toy_X2 = np.array([[2111.3, -2223., 404.0],\n",
    "                      [5222., -22221., 3.3]])\n",
    "    toy_y2 = np.array([[122.], [221.]])\n",
    "    toy_w3 = np.array([[-10.22], [-3.1]])\n",
    "    toy_X3 = np.array([[1.3, -1.2],\n",
    "                      [2.2, -2.1],\n",
    "                      [-2.3, -5.5],\n",
    "                      [3.2, 8.1],\n",
    "                      [3.3, -1.1],\n",
    "                      [-3.4, -2.22],\n",
    "                      [2.23, -4.4],\n",
    "                      [5.2, -2.3]])\n",
    "    toy_y3 = np.array([[10.3],\n",
    "                       [23.3],\n",
    "                       [10.1],\n",
    "                       [-20.2],\n",
    "                       [-10.2],\n",
    "                       [20.2],\n",
    "                       [-14.4],\n",
    "                       [-30.3]])\n",
    "    \n",
    "    assert grad_check(toy_X1, toy_y1, toy_w1)\n",
    "    assert grad_check(toy_X2, toy_y2, toy_w2)\n",
    "    assert grad_check(toy_X3, toy_y3, toy_w3)\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent\n",
    "\n",
    "A versão mais simples do algoritmo *gradient descent* faz uso de todas as observações do dataset de treinamento (esse algoritmo também é conhecido como *batch gradient descent* ou *vanilla gradient descent*).\n",
    "\n",
    "**Batch gradient descent**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Compute the gradient $\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 4)** \n",
    "Implemente o algoritmo batch gradient descent com a taxa de apreendizado fixa para a regressão linear. A função abaixo deve retornar três coisas: o vetor de pesos $\\mathbf{w}$, uma lista com cada peso obtido ao longo do treinamento, e uma lista com o custo de cada peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, w, learning_rate, num_iters):\n",
    "    \"\"\"\n",
    "     Performs batch gradient descent optimization.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    \n",
    "    weights_history = [w.flatten()]\n",
    "    cost_history = [compute_cost(X, y, w)]\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    learning_rate = 0.8\n",
    "    iterations = 20000\n",
    "    init = time.time()\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                              train_y_norm,\n",
    "                                                              initial_w,\n",
    "                                                              learning_rate,\n",
    "                                                              iterations)\n",
    "    assert cost_history[-1] < cost_history[0]\n",
    "    assert type(w) == np.ndarray\n",
    "    assert len(weights_history) == len(cost_history)\n",
    "    init = time.time() - init\n",
    "    print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "    print(\"Tem que ser em menos de 1 segundo \")\n",
    "    \n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Agora podemos treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    learning_rate = 0.03\n",
    "    iterations = 400\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                              train_y_norm,\n",
    "                                                              initial_w,\n",
    "                                                              learning_rate,\n",
    "                                                              iterations)\n",
    "    title = \"Optimization landscape\\nlearning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                                                  iterations)\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=title,\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history)\n",
    "    simple_step_plot([cost_history],\n",
    "                 \"loss\",\n",
    "                 'Training loss\\nlearning rate = {} | iterations = {}'.format(learning_rate,\n",
    "                                                                              iterations))\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiper parâmetros (*hyperparameters*)\n",
    "\n",
    "\n",
    "Hiper parâmetros são parâmetros que controlam o comportamento do algoritmo. Eles não são modificados pelo algoritmo de aprendizado. Escolhemos os hiper parâmetros de acordo com a performance deles no dataset de treinamento. Para evitar que o modelo decore o dataset de treinamento, pegamos uma parte desse dataset só para achar os melhores hiper parâmetros. Essa parte é chamada de **dataset de validação** (*validation set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    valid_X_norm = standardize(valid_X)\n",
    "    valid_y_norm = standardize(valid_y)\n",
    "    valid_X_1 = add_feature_ones(valid_X_norm)\n",
    "\n",
    "    hyper_params = [(0.001, 200),\n",
    "                    (0.1, 10),\n",
    "                    (0.9, 8),\n",
    "                    (0.02, 600)]\n",
    "\n",
    "    all_costs = []\n",
    "    all_w = []\n",
    "\n",
    "    for param in hyper_params:\n",
    "        learning_rate = param[0]\n",
    "        iterations = param[1]\n",
    "        w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                                  train_y_norm,\n",
    "                                                                  initial_w,\n",
    "                                                                  learning_rate,\n",
    "                                                                  iterations)\n",
    "        all_costs.append(compute_cost(valid_X_1, valid_y_norm, w))\n",
    "        all_w.append(w)\n",
    "        title = \"Optimization landscape\\n\"\n",
    "        title += \"learning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                               iterations)\n",
    "\n",
    "        plot_cost_function_curve(train_X_1,\n",
    "                                 train_y_norm,\n",
    "                                 compute_cost,\n",
    "                                 title=title,\n",
    "                                 weights_list=weights_history,\n",
    "                                 cost_list=cost_history)\n",
    "\n",
    "\n",
    "    best_result_i = np.argmin(all_costs)\n",
    "    best_w = all_w[best_result_i]\n",
    "    lowest_cost = all_costs[best_result_i]\n",
    "    best_params = hyper_params[best_result_i]\n",
    "\n",
    "    result_str = \"Best hyperparameters\\n\"\n",
    "    result_str += \"learning rate = {}\".format(best_params[0])\n",
    "    result_str += \" | iterations = {}\\n\".format(best_params[1])\n",
    "    result_str += \"w = {}\\n\".format(best_w.flatten())\n",
    "    result_str += \"lowest validation set cost = {}\\n\".format(lowest_cost)\n",
    "\n",
    "    print(result_str)\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Com o modelo treinado e escolhidos os melhores hiper parâmetros, podemos avaliá-lo sobre o dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_X_norm = standardize(test_X)\n",
    "    test_y_norm = standardize(test_y)\n",
    "    test_X_1 = add_feature_ones(test_X_norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prediction = linear_regression_prediction(test_X_1, best_w)\n",
    "    prediction = (prediction * np.std(train_y)) + np.mean(train_y)\n",
    "    r_2 = r_squared(test_y, prediction)\n",
    "\n",
    "    plot_points_regression(test_X,\n",
    "                           test_y,\n",
    "                           title='Test data',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$',\n",
    "                           prediction=prediction,\n",
    "                           r_squared=r_2,\n",
    "                           legend=True)\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente estocástico\n",
    "\n",
    "Nos casos em que $N$ é um número grande, computar $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ a cada iteração se torna algo muito custoso. Uma estratégia para lidar com isso é **aproximar** $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ usando o gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})} = \\nabla_{\\mathbf{w}}\\frac{1}{m}\\sum_{i=1}^{m} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $(\\mathbf{x}_{1}, y_{1}), \\dots ,(\\mathbf{x}_{m}, y_{m})$ é uma amostragem aleatória dos dados de treinamento. A  estocasticidade surge da escolha desses $m$ dados (para que $\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})}$ seja um estimador não enviesado de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ nós amostramos os $m$ dados a cada iteração). Normalmente usamos o nome **gradiente descendente estocástico** (*stochastic gradient descent* ou *online gradient descent*) quando $m=1$, e usamos o nome **minibatch stochastic gradient descent** quando $1 < m <N$ (nesse caso estamos usando apenas um pequeno lote dos dados, um *minibatch*). Usamos *batch* para referir a um *minibatch*, não confunda isso com *batch gradient descent*.\n",
    "\n",
    "\n",
    "**Stochastic gradient descent (SGD)**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Sample a minibatch of $m$ examples from the training data.\n",
    "    * Compute the gradient estimate $\\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 5)** \n",
    "Implemente o algoritmo stochastic gradient descent para a regressão linear com a taxa de apreendizado fixa. A saída da função é a mesma da função do exercício 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w, learning_rate, num_iters, batch_size):\n",
    "    \"\"\"\n",
    "     Performs stochastic gradient descent optimization\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :param batch_size: size of the minibatch\n",
    "    :type batch_size: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    init = time.time()\n",
    "    learning_rate = 0.8\n",
    "    iterations = 2000\n",
    "    batch_size = 36\n",
    "    w, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                                   train_y_norm,\n",
    "                                                                   initial_w,\n",
    "                                                                   learning_rate,\n",
    "                                                                   iterations,\n",
    "                                                                   batch_size)\n",
    "    assert cost_history[-1] < cost_history[0]\n",
    "    assert type(w) == np.ndarray\n",
    "    assert len(weights_history) == len(cost_history)\n",
    "    init = time.time() - init\n",
    "    print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "    print(\"Tem que ser em menos de 1.2 segundos\")\n",
    "    \n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Podemos experimentar com diferentes tamanhos de batch para ver que quanto maior o tamanho do batch (mais próximo de $N$) menor a variância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    hyper_params = [(0.001, 1000, 1),\n",
    "                    (0.001, 1000, 10),\n",
    "                    (0.001, 1000, 36)]\n",
    "    all_costs = []\n",
    "\n",
    "    for param in hyper_params:\n",
    "        learning_rate = param[0]\n",
    "        iterations = param[1]\n",
    "        batch_size = param[2]\n",
    "        _, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                                       train_y_norm,\n",
    "                                                                       initial_w,\n",
    "                                                                       learning_rate,\n",
    "                                                                       iterations,\n",
    "                                                                       batch_size)\n",
    "        all_costs.append(cost_history)\n",
    "        title = \"Optimization landscape\\n\"\n",
    "        title += \"learning rate = {}\".format(learning_rate)\n",
    "        title += \" | iterations = {}\".format(iterations)\n",
    "        title += \" | batch size = {}\".format(batch_size)\n",
    "        plot_cost_function_curve(train_X_1,\n",
    "                                 train_y_norm,\n",
    "                                 compute_cost,\n",
    "                                 title=title,\n",
    "                                 weights_list=weights_history,\n",
    "                                 cost_list=cost_history)\n",
    "\n",
    "\n",
    "    _, _, cost_history_full = batch_gradient_descent(train_X_1,\n",
    "                                                     train_y_norm,\n",
    "                                                     initial_w,\n",
    "                                                     learning_rate=0.001,\n",
    "                                                     num_iters=1000)\n",
    "\n",
    "    all_costs.append(cost_history_full)\n",
    "    labels_size = [\"batch size = \" + str(param[2]) for param in hyper_params]\n",
    "    labels_size += [\"batch size = \" + str(train_X_1.shape[0])]\n",
    "\n",
    "    simple_step_plot(all_costs,\n",
    "                     \"loss\",\n",
    "                     'Training loss',\n",
    "                      figsize=(8, 8),\n",
    "                      labels=labels_size)\n",
    "\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que normalizar?\n",
    "\n",
    "O primeiro motivo para se normalizar os dados é para evitar *overflow*. Também é verdade que quando não normalizamos os dados as *features* podem apresentar diferentes escalas -- note que esse é o caso nesse dataset em que uma *feature* só tem $1$s e a outra ($m^{2}$) apresenta bastante variação. Isso influencia no gradiente de modo que a cada atualização os valores dos pesos vão mudar de modo diferente mesmo usando o mesmo *learning rate*.\n",
    "\n",
    "Isso pode ser visto quando acompanhamos a mudança nos pesos ao longo do treinamento no dataset original e no normalizado. Note como o parâmetro $\\mathbf{w}[1]$ (que pondera a feature ($m^{2}$)) muda bem mais que o parâmetro $\\mathbf{w}[0]$ quando usamos o dataset não normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    _, weights_history_norm, cost_history_norm = batch_gradient_descent(train_X_1,\n",
    "                                                                        train_y_norm,\n",
    "                                                                        initial_w,\n",
    "                                                                        learning_rate,\n",
    "                                                                        10)\n",
    "\n",
    "    train_X_1_non_norm = add_feature_ones(train_X)\n",
    "\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1_non_norm,\n",
    "                                                              train_y,\n",
    "                                                              initial_w,\n",
    "                                                              0.000002,\n",
    "                                                              10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    w0_hist_norm = [w[0] for w in weights_history_norm]\n",
    "    w1_hist_norm = [w[1] for w in weights_history_norm]\n",
    "\n",
    "\n",
    "    w0mean, w0sdt, w0max, w0min = np.mean(w0_hist_norm), np.std(w0_hist_norm), np.max(w0_hist_norm), np.min(w0_hist_norm)\n",
    "    w1mean, w1sdt, w1max, w1min = np.mean(w0_hist_norm), np.std(w1_hist_norm), np.max(w1_hist_norm), np.min(w1_hist_norm)\n",
    "\n",
    "    print(\"\\nVariação dos pesos com o dataset normalizado\\n\")\n",
    "    print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "    print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))              \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    w0_hist = [w[0] for w in weights_history]\n",
    "    w1_hist = [w[1] for w in weights_history]\n",
    "\n",
    "\n",
    "    w0mean, w0sdt, w0max, w0min = np.mean(w0_hist), np.std(w0_hist), np.max(w0_hist), np.min(w0_hist)\n",
    "    w1mean, w1sdt, w1max, w1min = np.mean(w1_hist), np.std(w1_hist), np.max(w1_hist), np.min(w1_hist)\n",
    "\n",
    "    print(\"\\nVariação dos pesos com o dataset não normalizado\\n\")\n",
    "    print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "    print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Um dos resultados dessa atualização em scala diferente para cada feature é a não convergência do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:   \n",
    "    simple_step_plot([cost_history_norm],\n",
    "                     \"loss\",\n",
    "                     'Training loss (normalized)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    simple_step_plot([cost_history],\n",
    "                     \"loss\",\n",
    "                     'Training loss (non normalized)')\n",
    "\n",
    "\n",
    "\n",
    "    plot_cost_function_curve(train_X_1_non_norm,\n",
    "                             train_y,\n",
    "                             compute_cost,\n",
    "                             title=\"Optimization landscape\\n(non normalized data)\",\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history,\n",
    "                             range_points=(100, 100))\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais otimização!\n",
    "\n",
    "Há muitos outros algoritmos de otimização construídos em cima da ideia de gradiente descendente. Um bom resumo de alguns desses algoritimos pode ser encontrado [aqui](http://ruder.io/optimizing-gradient-descent/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
